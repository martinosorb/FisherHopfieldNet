{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Fisher Information for Hopfield Networks\n",
    "Here, we use sparse patterns in Hopfield Networks to demonstrate the overcoming of catastrophic forgetting.\n",
    "This notebook is used for comparing results of Normal Hebbian Learning (with a diminished learning rate such that the average learning rate fits the other cases) with learning where a certain number of weights are kept constant. The weights that are supposed to be kept constant are picked in 5 different ways:\n",
    "1. Highest Fisher information\n",
    "2. Lowest Fisher information\n",
    "3. Connections with largest absolute value of the weighs\n",
    "4. Connections where absolute value of weights exceeds a certain value\n",
    "5. Highest Fisher information calculated by 'Adapted Hebbian Learning Rule'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run -i '../code/import_libraries.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hopfieldNetwork import hopfieldNet\n",
    "from solverFile import solverClass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Hyperparameters\n",
    "Can be changed and adapted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ETA = 0.001 #learning rate\n",
    "NTRAIN = 1000 #number of epochs\n",
    "NUM_PATTERNS = 300 #number of patterns created\n",
    "SPARSITY = 0.1 #number of zeros: e.g. SPARSITY = 0.1 means 10% ones and 90% zeros\n",
    "IMAGE_SIZE = 10 #the size of our created pattern will be (IMAGE_SIZE x IMAGE_SIZE)\n",
    "# eval_f = 1 #evaluation frequency (every eval_f-th iteration) NOTE: currently not implemented\n",
    "TRIALS = 1 #number of trials over which the results will be averaged in order to get smooth results\n",
    "less_changed_weight_value = 0.00 #the learning rate of weights which are considered important have a learning\n",
    "#rate of ETA * less_changed_weight_value\n",
    "stored_patterns = 5 #number of patterns that are stored in the network before learning the new pattern\n",
    "number_of_changed_values = 4750 #the number of weigths that are changed is 2*number_of_changed_values (The factor of 2\n",
    "#is because of the symmetry of the weight matrix)\n",
    "RUN_LOCAL = True #if False, the 3. and 4. in the description will be discarded for higher computational speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a Pre-Test\n",
    "Output should be:\n",
    "The overall_error is:    0.0\n",
    "If not, set SPARSITY = 0.1 (script is not optimized regarding generality yet ;) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "netFisher = hopfieldNet(IMAGE_SIZE, ETA, SPARSITY)\n",
    "solver = solverClass()\n",
    "patterns = solver.create_patterns(SPARSITY, IMAGE_SIZE, NUM_PATTERNS)\n",
    "mean_value = SPARSITY\n",
    "original_patterns = copy.deepcopy(patterns)\n",
    "patterns = patterns - mean_value\n",
    "p = np.zeros(shape=(IMAGE_SIZE**2, IMAGE_SIZE**2))\n",
    "for i in range(int(stored_patterns)):\n",
    "    p += np.outer(patterns[:,i], patterns[:,i])\n",
    "    netFisher.append_pattern(patterns[:,i], NTRAIN)\n",
    "w1 = p/70\n",
    "\n",
    "#print(w1)\n",
    "\n",
    "netFisher.set_weights(w1)\n",
    "overall_error = 0\n",
    "for i in range(int(stored_patterns)):\n",
    "    netFisher.present_pattern(original_patterns[:,i])\n",
    "    netFisher.step(100)\n",
    "    output = netFisher.s\n",
    "    error = np.sum(original_patterns[:,i]-output)**2\n",
    "    overall_error += error\n",
    "print('The overall_error is:   ', overall_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dice coefficent as measure of accuracy\n",
    "As it can be seen, the results improve for all five cases, apart from the second one, where the results deteriorate.\n",
    "The measure of correctness is the Dice-coefficient, which is 2*cardinality(overlap(A,B)) / (cardinality(A) + cardinality(B)), with A being the results and B being the target for cells to be a 1.\n",
    "Hence, a Dice-coefficient of 1 means perfect memory, 0 means no memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dice_coefficient(p1, p2):\n",
    "    p = 2 * np.sum(np.floor(0.6*(p1 + p2)))\n",
    "    n = np.sum(p1)+np.sum(p2)\n",
    "    return p/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script to prove that Hebbian way of calculating Fisher Information is equal to variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = solverClass()\n",
    "patterns = solver.create_patterns(SPARSITY, IMAGE_SIZE, NUM_PATTERNS)\n",
    "netFisher = hopfieldNet(IMAGE_SIZE, ETA, SPARSITY)\n",
    "p = np.zeros(shape=(IMAGE_SIZE**2, IMAGE_SIZE**2))\n",
    "mean_value = SPARSITY\n",
    "original_patterns = copy.deepcopy(patterns)\n",
    "patterns = patterns - mean_value\n",
    "overall_pattern = np.zeros(shape = np.shape(patterns[:,0]))\n",
    "\n",
    "netFisher.calculate_fisher_information(patterns[:,0:stored_patterns])\n",
    "wF = netFisher.curvature\n",
    "netFisher.calculate_fisher_information_hebbian(patterns[:,0:stored_patterns])\n",
    "wH = netFisher.curvature\n",
    "a = np.array_equal(wF,wH)\n",
    "#print(wF)\n",
    "#print(wH)\n",
    "wD = wF - wH\n",
    "max_error = np.amax(np.abs(wD))\n",
    "summed_error = np.sum(wD**2)\n",
    "print('Max errro: ', max_error)\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(wF)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(wH)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traininig Phase\n",
    "Might take a couple of minutes. Output should look like:\n",
    "\n",
    "Started Learning\n",
    "\n",
    "Running trial  1  /  3 \n",
    "\n",
    "Running trial  2  /  3\n",
    "\n",
    "Running trial  3  /  3\n",
    "\n",
    "Finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i '../code/define_variables.py' #initializes all variables (mostly to zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_triangular(matrix):\n",
    "    return matrix[np.triu_indices_from(matrix, 1)]\n",
    "\n",
    "def from_triangular(size, arr, diagonal_value):\n",
    "    matrix = np.zeros([size, size])\n",
    "    matrix[np.triu_indices_from(matrix, 1)] = arr\n",
    "    matrix += matrix.T\n",
    "    np.fill_diagonal(matrix, diagonal_value)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%prun\n",
    "printmd('**Started Learning**')\n",
    "\n",
    "for trial in range(0, TRIALS):\n",
    "    if trial % eval_f == 0:\n",
    "        print('Running trial ', trial+1, ' / ', TRIALS)\n",
    "    solver = solverClass()\n",
    "    patterns = solver.create_patterns(SPARSITY, IMAGE_SIZE, NUM_PATTERNS)\n",
    "    netFisher = hopfieldNet(IMAGE_SIZE, ETA, SPARSITY)\n",
    "    p = np.zeros(shape=(IMAGE_SIZE**2, IMAGE_SIZE**2))\n",
    "    mean_value = SPARSITY\n",
    "    original_patterns = copy.deepcopy(patterns)\n",
    "    patterns = patterns - mean_value\n",
    "    overall_pattern = np.zeros(shape = np.shape(patterns[:,0]))\n",
    "\n",
    "    for i in range(int(stored_patterns)):\n",
    "        p += np.outer(patterns[:,i], patterns[:,i])\n",
    "        overall_pattern += (patterns[:,i]+mean_value)\n",
    "        netFisher.append_pattern(patterns[:,i], NTRAIN)\n",
    "    w1 = p/70\n",
    "\n",
    "    \n",
    "# ========== H ========== #\n",
    "    wF = w1\n",
    "    for epoch in range(NTRAIN):\n",
    "        diminish_lr = 2 * number_of_changed_values / (IMAGE_SIZE**2)**2\n",
    "        z = diminish_lr  * ETA * (np.outer(patterns[:,stored_patterns+1], patterns[:,stored_patterns+1]) - wF)\n",
    "        perturbation_vector = z\n",
    "        wF = wF + perturbation_vector\n",
    "        summed_perturbed0 += np.mean(np.abs(perturbation_vector))\n",
    "        netFisher.set_weights(wF)\n",
    "\n",
    "        overall_error = np.zeros(NTRAIN)\n",
    "        for i in range(int(stored_patterns)):\n",
    "            netFisher.present_pattern(original_patterns[:,i])\n",
    "            netFisher.step(100)\n",
    "            output = netFisher.s\n",
    "            error[epoch] = np.sum((original_patterns[:,i]-output)**2)\n",
    "            error[epoch] = dice_coefficient(original_patterns[:,i], output)\n",
    "            overall_error[epoch] += error[epoch]\n",
    "\n",
    "        netFisher.present_pattern(original_patterns[:,stored_patterns+1])\n",
    "        netFisher.step(100)\n",
    "        output = netFisher.s\n",
    "        error[epoch] = np.sum((original_patterns[:,stored_patterns+1]-output)**2)\n",
    "        error[epoch] = dice_coefficient(original_patterns[:,stored_patterns+1], output)\n",
    "\n",
    "        complete_error_mean[trial, epoch] = overall_error[epoch]/stored_patterns\n",
    "        complete_error_new_pattern[trial, epoch] = error[epoch]\n",
    "        \n",
    "    wH_final = netFisher.w\n",
    "    \n",
    "# ========= FL ========= #\n",
    "    if RUN_LOCAL == True:\n",
    "        #Now disturbing the weights\n",
    "        wF = w1\n",
    "        for epoch in range(NTRAIN):\n",
    "            z = ETA * (np.outer(patterns[:,stored_patterns+1], patterns[:,stored_patterns+1]) - wF)\n",
    "            netFisher.curvature = np.abs(w1)\n",
    "            weight_perturbation = less_changed_weight_value*np.ones(shape = np.shape(w1))\n",
    "            np.fill_diagonal(weight_perturbation, 1)\n",
    "            copied_curvature = copy.deepcopy(netFisher.curvature)\n",
    "            np.fill_diagonal(copied_curvature, 1000) #setting it to a very high value such that the diagonal is not touched\n",
    "            \n",
    "            copied_curvature_tri = to_triangular(copied_curvature) # Martino\n",
    "            weight_perturbation_tri = to_triangular(weight_perturbation)  # Martino\n",
    "            small_idx = np.argsort(copied_curvature_tri, axis=None)\n",
    "            weight_perturbation_tri[small_idx[:number_of_changed_values]] = 1\n",
    "            copied_curvature_tri[small_idx[:number_of_changed_values]] = 2000\n",
    "            weight_perturbation_v2 = from_triangular(IMAGE_SIZE**2, weight_perturbation_tri, 1)\n",
    "            copied_curvature_v2 = from_triangular(IMAGE_SIZE**2, copied_curvature_tri, 1000)\n",
    "\n",
    "#             for i in range(number_of_changed_values):\n",
    "#                 y_ind_current, x_ind_current = np.unravel_index(copied_curvature.argmin(), copied_curvature.shape)\n",
    "#                 weight_perturbation[y_ind_current, x_ind_current] = 1\n",
    "#                 weight_perturbation[x_ind_current, y_ind_current] = 1\n",
    "#                 copied_curvature[y_ind_current, x_ind_current] = 2000\n",
    "#                 copied_curvature[x_ind_current, y_ind_current] = 2000\n",
    "#             print(np.sum(np.abs(weight_perturbation - weight_perturbation_v2)))\n",
    "            weight_perturbation = weight_perturbation_v2\n",
    "            copied_curvature = copied_curvature_v2\n",
    "\n",
    "\n",
    "            if (epoch % 100) == 0:\n",
    "                print('The number of perturbed weights is (case 1):   ', np.sum(weight_perturbation))\n",
    "\n",
    "            xyz = np.zeros(np.shape(w1))\n",
    "            xyz[weight_perturbation == 1] = w1[weight_perturbation == 1]\n",
    "            mean_w1_considered2 += np.mean(np.abs(xyz))\n",
    "            perturbation_vector = weight_perturbation * z\n",
    "            wF = wF + perturbation_vector\n",
    "            summed_perturbed2 += np.mean(np.abs(perturbation_vector))\n",
    "            netFisher.set_weights(wF)\n",
    "\n",
    "            overall_errorFL = np.zeros(NTRAIN)\n",
    "            for i in range(int(stored_patterns)):\n",
    "                netFisher.present_pattern(original_patterns[:,i])\n",
    "                netFisher.step(100)\n",
    "                output = netFisher.s\n",
    "                error[epoch] = np.sum((original_patterns[:,i]-output)**2)\n",
    "                error[epoch] = dice_coefficient(original_patterns[:,i], output)\n",
    "                overall_errorFL[epoch] += error[epoch]\n",
    "\n",
    "            netFisher.present_pattern(original_patterns[:,stored_patterns+1])\n",
    "            netFisher.step(100)\n",
    "            output = netFisher.s\n",
    "            error[epoch] = np.sum((original_patterns[:,stored_patterns+1]-output)**2)\n",
    "            error[epoch] = dice_coefficient(original_patterns[:,stored_patterns+1], output)\n",
    "\n",
    "            complete_error_meanFL[trial, epoch] = overall_errorFL[epoch]/stored_patterns\n",
    "            complete_error_new_patternFL[trial, epoch] = error[epoch]\n",
    "\n",
    "            if epoch == 0:\n",
    "                x = np.abs(netFisher.w).flatten()\n",
    "                netFisher.curvature = np.abs(w1)\n",
    "                y = netFisher.curvature.flatten()\n",
    "                plt.figure(figsize=(5,5))\n",
    "                plt.plot(x,y, 'x')\n",
    "                plt.xlabel('abs(w)')\n",
    "                plt.ylabel('FisherInf')\n",
    "                plt.show()\n",
    "\n",
    "        wFL_final = netFisher.w\n",
    "\n",
    "# ======== FLT ========= #\n",
    "# Now disturbing the weights\n",
    "        wF = w1\n",
    "        for epoch in range(NTRAIN):\n",
    "            z = ETA * (np.outer(patterns[:,stored_patterns+1], patterns[:,stored_patterns+1]) - wF)\n",
    "            netFisher.curvature = np.abs(w1)\n",
    "            weight_perturbation = less_changed_weight_value*np.ones(shape = np.shape(w1))\n",
    "            np.fill_diagonal(weight_perturbation, 1)\n",
    "\n",
    "            weight_perturbation[np.abs(w1) < 0.008] = 1\n",
    "            np.fill_diagonal(weight_perturbation, 1)\n",
    "            if (epoch % 100) == 0:\n",
    "                print('The number of perturbed weights is (case 2):   ', np.sum(weight_perturbation))\n",
    "\n",
    "            xyz = np.zeros(np.shape(w1))\n",
    "            xyz[weight_perturbation == 1] = w1[weight_perturbation == 1]\n",
    "            mean_w1_considered2 += np.mean(np.abs(xyz))\n",
    "            perturbation_vector = weight_perturbation * z\n",
    "            wF = wF + perturbation_vector\n",
    "            summed_perturbed2 += np.mean(np.abs(perturbation_vector))\n",
    "            netFisher.set_weights(wF)\n",
    "\n",
    "            overall_errorFLT = np.zeros(NTRAIN)\n",
    "            for i in range(int(stored_patterns)):\n",
    "                netFisher.present_pattern(original_patterns[:,i])\n",
    "                netFisher.step(100)\n",
    "                output = netFisher.s\n",
    "                error[epoch] = np.sum((original_patterns[:,i]-output)**2)\n",
    "                error[epoch] = dice_coefficient(original_patterns[:,i], output)\n",
    "                overall_errorFLT[epoch] += error[epoch]\n",
    "\n",
    "            netFisher.present_pattern(original_patterns[:,stored_patterns+1])\n",
    "            netFisher.step(100)\n",
    "            output = netFisher.s\n",
    "            error[epoch] = np.sum((original_patterns[:,stored_patterns+1]-output)**2)\n",
    "            error[epoch] = dice_coefficient(original_patterns[:,stored_patterns+1], output)\n",
    "\n",
    "            complete_error_meanFLT[trial, epoch] = overall_errorFLT[epoch]/stored_patterns\n",
    "            complete_error_new_patternFLT[trial, epoch] = error[epoch]\n",
    "\n",
    "            #if epoch == (NTRAIN-1):\n",
    "            #    x = np.abs(netFisher.w).flatten()\n",
    "            #    netFisher.curvature = np.abs(w1)\n",
    "            #    y = netFisher.curvature.flatten()\n",
    "            #    plt.figure(figsize=(5,5))\n",
    "            #    plt.plot(x,y, 'x')\n",
    "            #    plt.xlabel('abs(w)')\n",
    "            #    plt.ylabel('FisherInf')\n",
    "            #    plt.show()\n",
    "\n",
    "        wFLT_final = netFisher.w\n",
    "    \n",
    "\n",
    "# ======== FI ======== #\n",
    "# Now disturbing the weights\n",
    "    wF = w1\n",
    "    for epoch in range(NTRAIN):\n",
    "        z = ETA * (np.outer(patterns[:,stored_patterns+1], patterns[:,stored_patterns+1]) - wF)\n",
    "        #netFisher.calculate_fisher_information(patterns[:,0:stored_patterns+1])\n",
    "        netFisher.calculate_fisher_information(patterns[:,0:stored_patterns])\n",
    "        weight_perturbation = less_changed_weight_value*np.ones(shape = np.shape(w1))\n",
    "        np.fill_diagonal(weight_perturbation, 1)\n",
    "        copied_curvature = copy.deepcopy(netFisher.curvature)\n",
    "        np.fill_diagonal(copied_curvature, 1000) # setting it to a very high value such that the diagonal is not touched\n",
    "        \n",
    "        copied_curvature_tri = to_triangular(copied_curvature) # Martino\n",
    "        weight_perturbation_tri = to_triangular(weight_perturbation)  # Martino\n",
    "        small_idx = np.argsort(copied_curvature_tri, axis=None)\n",
    "        weight_perturbation_tri[small_idx[:number_of_changed_values]] = 1\n",
    "        copied_curvature_tri[small_idx[:number_of_changed_values]] = 2000\n",
    "        weight_perturbation_v2 = from_triangular(IMAGE_SIZE**2, weight_perturbation_tri, 1)\n",
    "        copied_curvature_v2 = from_triangular(IMAGE_SIZE**2, copied_curvature_tri, 1000)\n",
    "            \n",
    "#         for i in range(number_of_changed_values):\n",
    "#             y_ind_current, x_ind_current = np.unravel_index(copied_curvature.argmin(), copied_curvature.shape)\n",
    "#             weight_perturbation[y_ind_current, x_ind_current] = 1\n",
    "#             weight_perturbation[x_ind_current, y_ind_current] = 1\n",
    "#             copied_curvature[y_ind_current, x_ind_current] = 2000\n",
    "#             copied_curvature[x_ind_current, y_ind_current] = 2000\n",
    "            \n",
    "#         print(np.sum(np.abs(weight_perturbation - weight_perturbation_v2)))\n",
    "        weight_perturbation = weight_perturbation_v2\n",
    "        copied_curvature = copied_curvature_v2\n",
    "\n",
    "\n",
    "        xyz = np.zeros(np.shape(w1))\n",
    "        xyz[weight_perturbation == 1] = w1[weight_perturbation == 1]\n",
    "        mean_w1_considered2 += np.mean(np.abs(xyz))\n",
    "        perturbation_vector = weight_perturbation * z\n",
    "        wF = wF + perturbation_vector\n",
    "        summed_perturbed2 += np.mean(np.abs(perturbation_vector))\n",
    "        netFisher.set_weights(wF)\n",
    "\n",
    "        overall_errorFI = np.zeros(NTRAIN)\n",
    "        for i in range(int(stored_patterns)):\n",
    "            netFisher.present_pattern(original_patterns[:,i])\n",
    "            netFisher.step(100)\n",
    "            output = netFisher.s\n",
    "            error[epoch] = np.sum((original_patterns[:,i]-output)**2)\n",
    "            error[epoch] = dice_coefficient(original_patterns[:,i], output)\n",
    "            overall_errorFI[epoch] += error[epoch]\n",
    "\n",
    "        netFisher.present_pattern(original_patterns[:,stored_patterns+1])\n",
    "        netFisher.step(100)\n",
    "        output = netFisher.s\n",
    "        error[epoch] = np.sum((original_patterns[:,stored_patterns+1]-output)**2)\n",
    "        error[epoch] = dice_coefficient(original_patterns[:,stored_patterns+1], output)\n",
    "\n",
    "        complete_error_meanFI[trial, epoch] = overall_errorFI[epoch]/stored_patterns\n",
    "        complete_error_new_patternFI[trial, epoch] = error[epoch]\n",
    "\n",
    "        #if epoch == (NTRAIN-1):\n",
    "        #    x = np.abs(netFisher.w).flatten()\n",
    "        #    netFisher.calculate_fisher_information(patterns[:,0:stored_patterns+1])\n",
    "        #    y = netFisher.curvature.flatten()\n",
    "        #    plt.figure(figsize=(5,5))\n",
    "        #    plt.plot(x,y, 'x')\n",
    "        #    plt.xlabel('abs(w)')\n",
    "        #    plt.ylabel('FisherInf')\n",
    "        #    plt.show()\n",
    "\n",
    "# ======== FIH ========= #    \n",
    "# Now disturbing the weights using hebbian way for fisher information\n",
    "    if RUN_LOCAL == True:\n",
    "        wF = w1\n",
    "        for epoch in range(NTRAIN):\n",
    "            z = ETA * (np.outer(patterns[:,stored_patterns+1], patterns[:,stored_patterns+1]) - wF)\n",
    "            #netFisher.calculate_fisher_information(patterns[:,0:stored_patterns+1])\n",
    "            netFisher.calculate_fisher_information_hebbian(patterns[:,0:stored_patterns])\n",
    "            weight_perturbation = less_changed_weight_value*np.ones(shape = np.shape(w1))\n",
    "            np.fill_diagonal(weight_perturbation, 1)\n",
    "            copied_curvature = copy.deepcopy(netFisher.curvature)\n",
    "            np.fill_diagonal(copied_curvature, 1000) #setting it to a very high value such that the diagonal is not touched\n",
    "            \n",
    "            copied_curvature_tri = to_triangular(copied_curvature) # Martino\n",
    "            weight_perturbation_tri = to_triangular(weight_perturbation)  # Martino\n",
    "            small_idx = np.argsort(copied_curvature_tri, axis=None)\n",
    "            weight_perturbation_tri[small_idx[:number_of_changed_values]] = 1\n",
    "            copied_curvature_tri[small_idx[:number_of_changed_values]] = 2000\n",
    "            weight_perturbation_v2 = from_triangular(IMAGE_SIZE**2, weight_perturbation_tri, 1)\n",
    "            copied_curvature_v2 = from_triangular(IMAGE_SIZE**2, copied_curvature_tri, 1000)\n",
    "            \n",
    "#             for i in range(number_of_changed_values):\n",
    "#                 y_ind_current, x_ind_current = np.unravel_index(copied_curvature.argmin(), copied_curvature.shape)\n",
    "#                 weight_perturbation[y_ind_current, x_ind_current] = 1\n",
    "#                 weight_perturbation[x_ind_current, y_ind_current] = 1\n",
    "#                 copied_curvature[y_ind_current, x_ind_current] = 2000\n",
    "#                 copied_curvature[x_ind_current, y_ind_current] = 2000\n",
    "#             print(np.sum(np.abs(weight_perturbation - weight_perturbation_v2)))\n",
    "            weight_perturbation = weight_perturbation_v2\n",
    "            copied_curvature = copied_curvature_v2\n",
    "\n",
    "\n",
    "            xyz = np.zeros(np.shape(w1))\n",
    "            xyz[weight_perturbation == 1] = w1[weight_perturbation == 1]\n",
    "            mean_w1_considered2 += np.mean(np.abs(xyz))\n",
    "            perturbation_vector = weight_perturbation * z\n",
    "            wF = wF + perturbation_vector\n",
    "            summed_perturbed2 += np.mean(np.abs(perturbation_vector))\n",
    "            netFisher.set_weights(wF)\n",
    "\n",
    "            overall_errorFIH = np.zeros(NTRAIN)\n",
    "            for i in range(int(stored_patterns)):\n",
    "                netFisher.present_pattern(original_patterns[:,i])\n",
    "                netFisher.step(100)\n",
    "                output = netFisher.s\n",
    "                error[epoch] = np.sum((original_patterns[:,i]-output)**2)\n",
    "                error[epoch] = dice_coefficient(original_patterns[:,i], output)\n",
    "                overall_errorFIH[epoch] += error[epoch]\n",
    "\n",
    "            netFisher.present_pattern(original_patterns[:,stored_patterns+1])\n",
    "            netFisher.step(100)\n",
    "            output = netFisher.s\n",
    "            error[epoch] = np.sum((original_patterns[:,stored_patterns+1]-output)**2)\n",
    "            error[epoch] = dice_coefficient(original_patterns[:,stored_patterns+1], output)\n",
    "\n",
    "            complete_error_meanFIH[trial, epoch] = overall_errorFIH[epoch]/stored_patterns\n",
    "            complete_error_new_patternFIH[trial, epoch] = error[epoch]\n",
    "\n",
    "            #if epoch == (NTRAIN-1):\n",
    "            #    x = np.abs(netFisher.w).flatten()\n",
    "            #    netFisher.calculate_fisher_information(patterns[:,0:stored_patterns+1])\n",
    "            #    y = netFisher.curvature.flatten()\n",
    "            #    plt.figure(figsize=(5,5))\n",
    "            #    plt.plot(x,y, 'x')\n",
    "            #    plt.xlabel('abs(w)')\n",
    "            #    plt.ylabel('FisherInf')\n",
    "            #    plt.show()\n",
    "\n",
    "        wFIH_final = netFisher.w\n",
    "printmd('**Finished**')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "savedict = {\n",
    "    'trad_80_O': complete_error_mean,\n",
    "    'trad_80_N': complete_error_new_pattern,\n",
    "    'FL_O': complete_error_meanFL,\n",
    "    'FL_N': complete_error_new_patternFL,\n",
    "    'FLT_O': complete_error_meanFLT,\n",
    "    'FLT_N': complete_error_new_patternFLT,\n",
    "    'FI_O': complete_error_meanFI,\n",
    "    'FI_N': complete_error_new_patternFI,\n",
    "    'FIH_O': complete_error_meanFIH,\n",
    "    'FIH_N': complete_error_new_patternFIH }\n",
    "\n",
    "np.savez(\"Complete_errors2.npz\", **savedict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "savedict = np.load(\"Complete_errors.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stuff(what, label):\n",
    "    global axisn\n",
    "    plt.subplot(5,2,axisn)\n",
    "    axisn += 1\n",
    "    center = np.median(what, axis=0)\n",
    "    edges = np.percentile(what, [25, 75], axis=0)\n",
    "    plt.plot(center, label=label)\n",
    "    plt.fill_between(np.arange(len(center)), *edges, alpha=.3)\n",
    "    plt.ylabel('Pixel Error %')\n",
    "    plt.ylim([0, 1.1])\n",
    "    plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.subplots(2, 5, figsize=(20,20))\n",
    "\n",
    "keys = ['trad_80_O', 'trad_80_N', 'FL_O', 'FL_N', 'FLT_O', 'FLT_N', 'FI_O', 'FI_N', 'FIH_O', 'FIH_N']\n",
    "\n",
    "axisn = 1\n",
    "for k in keys:\n",
    "    plot_stuff(savedict[k], label=k)\n",
    "\n",
    "# axisn = 1\n",
    "# plot_stuff(savedict[keys[axisn-1]], 'Perturbing all weights equally with only 80% learning rate')\n",
    "# plt.title('Old pattern',fontweight='bold', fontsize=20)\n",
    "\n",
    "# plot_stuff(savedict[keys[axisn-1]], 'Perturbing all weights equally with only 80% learning rate')\n",
    "# plt.title('New pattern',fontweight='bold', fontsize=20)\n",
    "\n",
    "# if RUN_LOCAL == True:\n",
    "#     plot_stuff(savedict[keys[axisn-1]], label = 'Perturbing all weights using a Local Rule')\n",
    "#     plot_stuff(savedict[keys[axisn-1]], label = 'Perturbing all weights using a Local Rule')\n",
    "#     plot_stuff(savedict[keys[axisn-1]], label = 'Perturbing all weights using an entirely Local Rule')\n",
    "#     plot_stuff(savedict[keys[axisn-1]], label = 'Perturbing all weights using an entirely Local Rule')\n",
    "    \n",
    "# plot_stuff(savedict[keys[axisn-1]], label = 'Perturbing all weights using Fisher information')\n",
    "# plot_stuff(savedict[keys[axisn-1]], label = 'Perturbing all weights using Fisher information')\n",
    "\n",
    "# if RUN_LOCAL == True:\n",
    "#     plot_stuff(savedict[keys[axisn-1]], label = 'Perturbing all weights using Fisher information Hebbian')\n",
    "#     plot_stuff(savedict[keys[axisn-1]], label = 'Perturbing all weights using Fisher information Hebbian')\n",
    "    \n",
    "plt.xlabel(\"n. Iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,8))\n",
    "plt.subplot(1,2,1)\n",
    "\n",
    "old_H, = plt.plot(np.mean(complete_error_mean, axis = 0), label = 'Perturbing all weights equally with only %f learning rate'%diminish_lr)\n",
    "old_FL, = plt.plot(np.mean(complete_error_meanFL, axis = 0), label = 'Perturbing all weights using a Local Rule')\n",
    "old_FLT, = plt.plot(np.mean(complete_error_meanFLT, axis = 0), label = 'Perturbing all weights using an entirely Local Rule')\n",
    "old_FI, = plt.plot(np.mean(complete_error_meanFI, axis = 0), label = 'Perturbing all weights using Fisher information')\n",
    "old_FIH, = plt.plot(np.mean(complete_error_meanFIH, axis = 0), label = 'Perturbing all weights using Fisher information Hebbian')\n",
    "\n",
    "plt.title('Old pattern',fontweight='bold', fontsize=20)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Dice Coefficient')\n",
    "plt.legend(handles = [old_H, old_FL, old_FLT, old_FI, old_FIH], bbox_to_anchor=(1, -0.1), loc=1, borderaxespad=0.)\n",
    "\n",
    "display_values_until = 3000\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "a = np.mean(complete_error_new_pattern, axis = 0)\n",
    "b = np.mean(complete_error_new_patternFL, axis = 0)\n",
    "c = np.mean(complete_error_new_patternFLT, axis = 0)\n",
    "d = np.mean(complete_error_new_patternFI, axis = 0)\n",
    "e = np.mean(complete_error_new_patternFIH, axis = 0)\n",
    "\n",
    "new_H, = plt.plot(a[0:display_values_until], label = 'Perturbing all weights equally with only %f learning rate'%diminish_lr)\n",
    "new_FL, = plt.plot(b[0:display_values_until], label = 'Perturbing all weights using a Local Rule')\n",
    "new_FLT, = plt.plot(c[0:display_values_until], label = 'Perturbing all weights using an entirely Local Rule')\n",
    "new_FI, = plt.plot(d[0:display_values_until], label = 'Perturbing all weights using Fisher information')\n",
    "new_FIH, = plt.plot(e[0:display_values_until], label = 'Perturbing all weights using Fisher information Hebbian')\n",
    "\n",
    "plt.title('New pattern',fontweight='bold', fontsize=20)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Dice Coefficient')\n",
    "plt.legend(handles = [new_H, new_FL, new_FLT, new_FI, new_FIH], bbox_to_anchor=(1, -0.1), loc=1, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_stuff_together(what, label, axisn):\n",
    "    plt.subplot(1, 2, axisn)\n",
    "    center = np.median(what, axis=0)\n",
    "    edges = np.percentile(what, [25, 75], axis=0)\n",
    "    l = plt.plot(center, label=label, linewidth=2.5)\n",
    "    c = l[0].get_color()\n",
    "    plt.fill_between(np.arange(len(center)), *edges, alpha=.15, color=c)\n",
    "    plt.plot(edges.T, color=c, alpha=.2)\n",
    "    plt.ylabel('Pixel Error %')\n",
    "    plt.ylim([0, 1.1])\n",
    "    plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,8))\n",
    "\n",
    "for k in keys:\n",
    "    axisn = 2 if k[-1] == 'N' else 1\n",
    "    plot_stuff_together(savedict[k], label=k, axisn=axisn)\n",
    "    \n",
    "plt.subplot(122)\n",
    "plt.title('New pattern',fontweight='bold', fontsize=20)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Dice Coefficient')\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title('Old pattern',fontweight='bold', fontsize=20)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Dice Coefficient')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
