{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Fisher Information for Hopfield Networks\n",
    "Here, we use sparse patterns in Hopfield Networks to demonstrate the overcoming of catastrophic forgetting.\n",
    "This notebook is used for comparing results of Normal Hebbian Learning (with a diminished learning rate such that the average learning rate fits the other cases) with learning where a certain number of weights are kept constant. The weights that are supposed to be kept constant are picked in 5 different ways:\n",
    "1. Highest Fisher information\n",
    "2. Lowest Fisher information\n",
    "3. Connections with largest absolute value of the weighs\n",
    "4. Connections where absolute value of weights exceeds a certain value\n",
    "5. Highest Fisher information calculated by 'Adapted Hebbian Learning Rule'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "# import random\n",
    "# import math\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "# from scipy.special import expit\n",
    "# from scipy.misc import imresize\n",
    "import sys\n",
    "sys.path.append('../code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from hopfieldNetwork import hopfieldNet\n",
    "from solverFile import solverClass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Hyperparameters\n",
    "Can be changed and adapted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ETA = 0.001 # learning rate\n",
    "NTRAIN = 2000 # number of epochs\n",
    "NUM_PATTERNS = 1000 # number of patterns created\n",
    "SPARSITY = 0.1 # number of zeros: e.g. SPARSITY = 0.1 means 10% ones and 90% zeros\n",
    "IMAGE_SIZE = 10 # the size of our created pattern will be (IMAGE_SIZE x IMAGE_SIZE)\n",
    "eval_f = 1 # evaluation frequency (every eval_f-th iteration) NOTE: currently not implemented\n",
    "TRIALS = 300 # number of trials over which the results will be averaged in order to get smooth results\n",
    "less_changed_weight_value = 0.00 # the learning rate of weights which are considered important have a\n",
    "                                 # learning rate of ETA * less_changed_weight_value\n",
    "stored_patterns = 10 # number of patterns that are stored in the network before learning the new pattern\n",
    "number_of_changed_values = 4750 # the number of weigths that are changed is 2*number_of_changed_values\n",
    "                                # (The factor of 2 is because of the symmetry of the weight matrix)\n",
    "\n",
    "# whether to run experiments with different learning rules\n",
    "RUN_FL = False  # similar to FLT, I prefer the local version\n",
    "RUN_FIH = False  # the experiment shows it's the same...\n",
    "RUN_FLT = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a Pre-Test\n",
    "Output should be:\n",
    "The overall_error is:    0.0\n",
    "If not, set SPARSITY = 0.1 (script is not optimized regarding generality yet ;) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "netFisher = hopfieldNet(IMAGE_SIZE, ETA, SPARSITY)\n",
    "solver = solverClass()\n",
    "patterns = solver.create_patterns(SPARSITY, IMAGE_SIZE, NUM_PATTERNS)\n",
    "mean_value = SPARSITY\n",
    "original_patterns = copy.deepcopy(patterns)\n",
    "patterns = patterns - mean_value\n",
    "p = np.zeros(shape=(IMAGE_SIZE**2, IMAGE_SIZE**2))\n",
    "for i in range(int(stored_patterns)):\n",
    "    p += np.outer(patterns[:,i], patterns[:,i])\n",
    "    netFisher.append_pattern(patterns[:,i], NTRAIN)\n",
    "w1 = p/70\n",
    "\n",
    "#print(w1)\n",
    "\n",
    "netFisher.set_weights(w1)\n",
    "overall_error = 0\n",
    "for i in range(int(stored_patterns)):\n",
    "    netFisher.present_pattern(original_patterns[:,i])\n",
    "    netFisher.step(100)\n",
    "    output = netFisher.s\n",
    "    error = np.sum(original_patterns[:,i]-output)**2\n",
    "    overall_error += error\n",
    "print('The overall_error is:   ', overall_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dice coefficent as measure of accuracy\n",
    "As it can be seen, the results improve for all five cases, apart from the second one, where the results deteriorate.\n",
    "The measure of correctness is the Dice-coefficient, which is 2*cardinality(overlap(A,B)) / (cardinality(A) + cardinality(B)), with A being the results and B being the target for cells to be a 1.\n",
    "Hence, a Dice-coefficient of 1 means perfect memory, 0 means no memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dice_coefficient(p1, p2):\n",
    "    p = 2 * np.sum(np.floor(0.6*(p1 + p2)))\n",
    "    n = np.sum(p1)+np.sum(p2)\n",
    "    return p/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script to prove that Hebbian way of calculating Fisher Information is equal to variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_triangular(matrix):\n",
    "    return matrix[np.triu_indices_from(matrix, 1)]\n",
    "\n",
    "def from_triangular(size, arr, diagonal_value):\n",
    "    matrix = np.zeros([size, size])\n",
    "    matrix[np.triu_indices_from(matrix, 1)] = arr\n",
    "    matrix += matrix.T\n",
    "    np.fill_diagonal(matrix, diagonal_value)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "solver = solverClass()\n",
    "patterns = solver.create_patterns(SPARSITY, IMAGE_SIZE, NUM_PATTERNS)\n",
    "original_patterns = copy.deepcopy(patterns)\n",
    "patterns = patterns - SPARSITY\n",
    "\n",
    "FIM = {}\n",
    "FIMdiag = {}\n",
    "stpatts = [5, 10, 50, 100, 200]\n",
    "\n",
    "for stpatt in stpatts:\n",
    "    print(\"Storing\", stpatt, \"patterns\")\n",
    "    netFisher = hopfieldNet(IMAGE_SIZE, ETA, SPARSITY)\n",
    "\n",
    "\n",
    "    # Training the network (NOT NEEDED for curvature)\n",
    "    p = np.zeros(shape=(IMAGE_SIZE**2, IMAGE_SIZE**2))\n",
    "    overall_pattern = np.zeros(shape = np.shape(patterns[:,0]))\n",
    "\n",
    "    for i in range(int(stpatt)):\n",
    "        p += np.outer(patterns[:,i], patterns[:,i])\n",
    "        overall_pattern += (patterns[:,i]+SPARSITY)\n",
    "        netFisher.append_pattern(patterns[:,i], NTRAIN)\n",
    "    netFisher.set_weights(p/70)\n",
    "\n",
    "    # evaluating pattern storage\n",
    "    overall_error = 0\n",
    "    for i in range(int(stpatt)):\n",
    "        netFisher.present_pattern(original_patterns[:,i])\n",
    "        netFisher.step(100)\n",
    "        output = netFisher.s\n",
    "        error = np.sum(original_patterns[:,i]-output)**2\n",
    "        overall_error += error\n",
    "    print('The overall_error is:   ', overall_error)\n",
    "\n",
    "    # Computing curvature\n",
    "    netFisher.calculate_fisher_information(patterns[:,0:stpatt])\n",
    "    wF = netFisher.curvature\n",
    "    netFisher.calculate_fisher_information_hebbian(patterns[:,0:stpatt])\n",
    "    wH = netFisher.curvature\n",
    "    FIMdiag[stpatt] = wF\n",
    "    wD = wF - wH\n",
    "    max_error = np.amax(np.abs(wD))\n",
    "    summed_error = np.sum(np.abs(wD))\n",
    "    fim_complete = netFisher.calculate_complete_fisher_information(patterns[:,0:stpatt])\n",
    "    FIM[stpatt] = fim_complete\n",
    "    print('Max error: ', max_error, \"Summed error:\", summed_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 10))\n",
    "i = 1\n",
    "for stpatt in stpatts:\n",
    "    fimdiag = to_triangular(FIMdiag[stpatt])\n",
    "    print(len(fimdiag))\n",
    "    plt.subplot(len(stpatts), 1, i)\n",
    "    plt.hist(fimdiag, bins=np.linspace(0, 20, 50), color='C'+str(i-1), label=str(stpatt))\n",
    "    #plt.xlim([-2, 25])\n",
    "    #plt.xscale('log')\n",
    "    if i == 1:\n",
    "        plt.title(\"Sloppiness in a Hopfield network (s: {}, N: {})\".format(\n",
    "                  SPARSITY, IMAGE_SIZE**2))\n",
    "    i += 1\n",
    "    plt.legend()\n",
    "    #plt.yscale('log')\n",
    "#     fimdiag.sort()\n",
    "#     plt.plot(fimdiag[::-1], '.', label=stpatt)\n",
    "\n",
    "#plt.xlim([-2, 1000])\n",
    "#plt.ylim([.1, 100])\n",
    "#plt.ylabel(\"Diagonal element of Fisher information\")\n",
    "#plt.xlabel(\"Element rank rank\")\n",
    "\n",
    "#plt.legend(title=\"Number of \\npatterns stored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FIMeig = {stpatt:np.linalg.eigvalsh(FIM[stpatt])[::-1] for stpatt in stpatts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 10))\n",
    "i = 1\n",
    "for stpatt in stpatts:\n",
    "    # looking at the diagonal yields very similar results to the above (albeit not identical)\n",
    "    # fimdiag = np.diagonal(FIM[stpatt])/np.mean(np.diagonal(FIM[stpatt]))\n",
    "    eig = FIMeig[stpatt]\n",
    "    plt.subplot(len(stpatts), 1, i)\n",
    "    plt.hist(eig, bins=np.linspace(0, 10, 50), color='C'+str(i-1), label=str(stpatt))\n",
    "    #plt.xlim([-2, 25])\n",
    "    #plt.xscale('log')\n",
    "    if i == 1:\n",
    "        plt.title(\"Sloppiness in a Hopfield network (s: {}, N: {})\".format(\n",
    "                  SPARSITY, IMAGE_SIZE**2))\n",
    "    i += 1\n",
    "    plt.legend()\n",
    "    plt.yscale('log')\n",
    "#     fimdiag.sort()\n",
    "#     plt.plot(fimdiag[::-1], '.', label=stpatt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for stpatt in stpatts:\n",
    "    # looking at the diagonal yields very similar results to the above (albeit not identical)\n",
    "    # fimdiag = np.diagonal(FIM[stpatt])/np.mean(np.diagonal(FIM[stpatt]))\n",
    "    eig = FIMeig[stpatt]\n",
    "    plt.plot(eig, '.', label=str(stpatt))\n",
    "    plt.yscale('log')\n",
    "plt.xlim([-5, 200])\n",
    "plt.ylim([7e-2, 12])\n",
    "plt.ylabel(\"Eigenvalue of Fisher information\")\n",
    "plt.xlabel(\"Eigenvalue rank\")\n",
    "\n",
    "plt.legend(title=\"Number of \\npatterns stored\")\n",
    "plt.title(\"Sloppiness in a Hopfield network (s: {}, N: {})\".format(\n",
    "                  SPARSITY, IMAGE_SIZE**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numeigs = [ (FIMeig[s] > 0.1).sum() for s in stpatts]\n",
    "plt.plot(stpatts, numeigs, '.')\n",
    "print(numeigs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Phase\n",
    "Might take a couple of minutes. Output should look like:\n",
    "\n",
    "Started Learning\n",
    "\n",
    "Running trial  1  /  3 \n",
    "\n",
    "Running trial  2  /  3\n",
    "\n",
    "Running trial  3  /  3\n",
    "\n",
    "Finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run -i '../code/define_variables.py' #initializes all variables (mostly to zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('**Started Learning**')\n",
    "\n",
    "for trial in range(0, TRIALS):\n",
    "    if trial % eval_f == 0:\n",
    "        print('Running trial ', trial+1, '/', TRIALS)\n",
    "    solver = solverClass()\n",
    "    patterns = solver.create_patterns(SPARSITY, IMAGE_SIZE, NUM_PATTERNS)\n",
    "    netFisher = hopfieldNet(IMAGE_SIZE, ETA, SPARSITY)\n",
    "    p = np.zeros(shape=(IMAGE_SIZE**2, IMAGE_SIZE**2))\n",
    "    mean_value = SPARSITY\n",
    "    original_patterns = copy.deepcopy(patterns)\n",
    "    patterns = patterns - mean_value\n",
    "    overall_pattern = np.zeros(shape = np.shape(patterns[:,0]))\n",
    "\n",
    "    for i in range(int(stored_patterns)):\n",
    "        p += np.outer(patterns[:,i], patterns[:,i])\n",
    "        overall_pattern += (patterns[:,i]+mean_value)\n",
    "        netFisher.append_pattern(patterns[:,i], NTRAIN)\n",
    "    w1 = p/70\n",
    "\n",
    "    \n",
    "# ========== H ========== #\n",
    "    wF = w1\n",
    "    for epoch in range(NTRAIN):\n",
    "        diminish_lr = 2 * number_of_changed_values / (IMAGE_SIZE**2)**2\n",
    "        z = diminish_lr  * ETA * (np.outer(patterns[:,stored_patterns+1], patterns[:,stored_patterns+1]) - wF)\n",
    "        perturbation_vector = z\n",
    "        wF = wF + perturbation_vector\n",
    "        summed_perturbed0 += np.mean(np.abs(perturbation_vector))\n",
    "        netFisher.set_weights(wF)\n",
    "\n",
    "        overall_error = np.zeros(NTRAIN)\n",
    "        for i in range(int(stored_patterns)):\n",
    "            netFisher.present_pattern(original_patterns[:,i])\n",
    "            netFisher.step(100)\n",
    "            output = netFisher.s\n",
    "            error[epoch] = np.sum((original_patterns[:,i]-output)**2)\n",
    "            error[epoch] = dice_coefficient(original_patterns[:,i], output)\n",
    "            overall_error[epoch] += error[epoch]\n",
    "\n",
    "        netFisher.present_pattern(original_patterns[:,stored_patterns+1])\n",
    "        netFisher.step(100)\n",
    "        output = netFisher.s\n",
    "        error[epoch] = np.sum((original_patterns[:,stored_patterns+1]-output)**2)\n",
    "        error[epoch] = dice_coefficient(original_patterns[:,stored_patterns+1], output)\n",
    "\n",
    "        complete_error_mean[trial, epoch] = overall_error[epoch]/stored_patterns\n",
    "        complete_error_new_pattern[trial, epoch] = error[epoch]\n",
    "        \n",
    "    wH_final = netFisher.w\n",
    "    \n",
    "# ========= FL ========= #\n",
    "    if RUN_FL:\n",
    "        #Now disturbing the weights\n",
    "        wF = w1\n",
    "        for epoch in range(NTRAIN):\n",
    "            z = ETA * (np.outer(patterns[:,stored_patterns+1], patterns[:,stored_patterns+1]) - wF)\n",
    "            netFisher.curvature = np.abs(w1)  # not an actual curvature\n",
    "            weight_perturbation = less_changed_weight_value*np.ones(shape = np.shape(w1))\n",
    "            np.fill_diagonal(weight_perturbation, 1)\n",
    "            copied_curvature = copy.deepcopy(netFisher.curvature)  # not an actual curvature\n",
    "            np.fill_diagonal(copied_curvature, 1000) #setting it to a very high value such that the diagonal is not touched\n",
    "            \n",
    "            copied_curvature_tri = to_triangular(copied_curvature) # Martino\n",
    "            weight_perturbation_tri = to_triangular(weight_perturbation)  # Martino\n",
    "            small_idx = np.argsort(copied_curvature_tri, axis=None)\n",
    "            weight_perturbation_tri[small_idx[:number_of_changed_values]] = 1\n",
    "            copied_curvature_tri[small_idx[:number_of_changed_values]] = 2000\n",
    "            weight_perturbation_v2 = from_triangular(IMAGE_SIZE**2, weight_perturbation_tri, 1)\n",
    "            copied_curvature_v2 = from_triangular(IMAGE_SIZE**2, copied_curvature_tri, 1000)\n",
    "\n",
    "#             for i in range(number_of_changed_values):\n",
    "#                 y_ind_current, x_ind_current = np.unravel_index(copied_curvature.argmin(), copied_curvature.shape)\n",
    "#                 weight_perturbation[y_ind_current, x_ind_current] = 1\n",
    "#                 weight_perturbation[x_ind_current, y_ind_current] = 1\n",
    "#                 copied_curvature[y_ind_current, x_ind_current] = 2000\n",
    "#                 copied_curvature[x_ind_current, y_ind_current] = 2000\n",
    "#             print(np.sum(np.abs(weight_perturbation - weight_perturbation_v2)))\n",
    "            weight_perturbation = weight_perturbation_v2\n",
    "            copied_curvature = copied_curvature_v2\n",
    "\n",
    "\n",
    "            if (epoch % 100) == 0:\n",
    "                print('The number of perturbed weights is (case 1):   ', np.sum(weight_perturbation))\n",
    "\n",
    "            xyz = np.zeros(np.shape(w1))\n",
    "            xyz[weight_perturbation == 1] = w1[weight_perturbation == 1]\n",
    "            mean_w1_considered2 += np.mean(np.abs(xyz))\n",
    "            perturbation_vector = weight_perturbation * z\n",
    "            wF = wF + perturbation_vector\n",
    "            summed_perturbed2 += np.mean(np.abs(perturbation_vector))\n",
    "            netFisher.set_weights(wF)\n",
    "\n",
    "            overall_errorFL = np.zeros(NTRAIN)\n",
    "            for i in range(int(stored_patterns)):\n",
    "                netFisher.present_pattern(original_patterns[:,i])\n",
    "                netFisher.step(100)\n",
    "                output = netFisher.s\n",
    "                error[epoch] = np.sum((original_patterns[:,i]-output)**2)\n",
    "                error[epoch] = dice_coefficient(original_patterns[:,i], output)\n",
    "                overall_errorFL[epoch] += error[epoch]\n",
    "\n",
    "            netFisher.present_pattern(original_patterns[:,stored_patterns+1])\n",
    "            netFisher.step(100)\n",
    "            output = netFisher.s\n",
    "            error[epoch] = np.sum((original_patterns[:,stored_patterns+1]-output)**2)\n",
    "            error[epoch] = dice_coefficient(original_patterns[:,stored_patterns+1], output)\n",
    "\n",
    "            complete_error_meanFL[trial, epoch] = overall_errorFL[epoch]/stored_patterns\n",
    "            complete_error_new_patternFL[trial, epoch] = error[epoch]\n",
    "\n",
    "            if epoch == 0:\n",
    "                x = np.abs(netFisher.w).flatten()\n",
    "                netFisher.curvature = np.abs(w1)\n",
    "                y = netFisher.curvature.flatten()\n",
    "                plt.figure(figsize=(5,5))\n",
    "                plt.plot(x,y, 'x')\n",
    "                plt.xlabel('abs(w)')\n",
    "                plt.ylabel('FisherInf')\n",
    "                plt.show()\n",
    "\n",
    "        wFL_final = netFisher.w\n",
    "\n",
    "# ======== FLT ========= #\n",
    "# Now disturbing the weights\n",
    "    if RUN_FLT:\n",
    "        wF = w1\n",
    "        for epoch in range(NTRAIN):\n",
    "            z = ETA * (np.outer(patterns[:,stored_patterns+1], patterns[:,stored_patterns+1]) - wF)\n",
    "            netFisher.curvature = np.abs(w1)\n",
    "            weight_perturbation = less_changed_weight_value*np.ones(shape = np.shape(w1))\n",
    "            np.fill_diagonal(weight_perturbation, 1)\n",
    "\n",
    "            weight_perturbation[np.abs(w1) < 0.008] = 1\n",
    "            np.fill_diagonal(weight_perturbation, 1)\n",
    "            if (epoch % 100) == 0:\n",
    "                print('The number of perturbed weights is (case 2):   ', np.sum(weight_perturbation))\n",
    "\n",
    "            xyz = np.zeros(np.shape(w1))\n",
    "            xyz[weight_perturbation == 1] = w1[weight_perturbation == 1]\n",
    "            mean_w1_considered2 += np.mean(np.abs(xyz))\n",
    "            perturbation_vector = weight_perturbation * z\n",
    "            wF = wF + perturbation_vector\n",
    "            summed_perturbed2 += np.mean(np.abs(perturbation_vector))\n",
    "            netFisher.set_weights(wF)\n",
    "\n",
    "            overall_errorFLT = np.zeros(NTRAIN)\n",
    "            for i in range(int(stored_patterns)):\n",
    "                netFisher.present_pattern(original_patterns[:,i])\n",
    "                netFisher.step(100)\n",
    "                output = netFisher.s\n",
    "                error[epoch] = np.sum((original_patterns[:,i]-output)**2)\n",
    "                error[epoch] = dice_coefficient(original_patterns[:,i], output)\n",
    "                overall_errorFLT[epoch] += error[epoch]\n",
    "\n",
    "            netFisher.present_pattern(original_patterns[:,stored_patterns+1])\n",
    "            netFisher.step(100)\n",
    "            output = netFisher.s\n",
    "            error[epoch] = np.sum((original_patterns[:,stored_patterns+1]-output)**2)\n",
    "            error[epoch] = dice_coefficient(original_patterns[:,stored_patterns+1], output)\n",
    "\n",
    "            complete_error_meanFLT[trial, epoch] = overall_errorFLT[epoch]/stored_patterns\n",
    "            complete_error_new_patternFLT[trial, epoch] = error[epoch]\n",
    "\n",
    "            #if epoch == (NTRAIN-1):\n",
    "            #    x = np.abs(netFisher.w).flatten()\n",
    "            #    netFisher.curvature = np.abs(w1)\n",
    "            #    y = netFisher.curvature.flatten()\n",
    "            #    plt.figure(figsize=(5,5))\n",
    "            #    plt.plot(x,y, 'x')\n",
    "            #    plt.xlabel('abs(w)')\n",
    "            #    plt.ylabel('FisherInf')\n",
    "            #    plt.show()\n",
    "\n",
    "        wFLT_final = netFisher.w\n",
    "    \n",
    "\n",
    "# ======== FI ======== #\n",
    "# Now disturbing the weights\n",
    "    wF = w1\n",
    "    for epoch in range(NTRAIN):\n",
    "        z = ETA * (np.outer(patterns[:,stored_patterns+1], patterns[:,stored_patterns+1]) - wF)\n",
    "        #netFisher.calculate_fisher_information(patterns[:,0:stored_patterns+1])\n",
    "        netFisher.calculate_fisher_information(patterns[:,0:stored_patterns])\n",
    "        weight_perturbation = less_changed_weight_value*np.ones(shape = np.shape(w1))\n",
    "        np.fill_diagonal(weight_perturbation, 1)\n",
    "        copied_curvature = copy.deepcopy(netFisher.curvature)\n",
    "        np.fill_diagonal(copied_curvature, 1000) # setting it to a very high value such that the diagonal is not touched\n",
    "        \n",
    "        copied_curvature_tri = to_triangular(copied_curvature) # Martino\n",
    "        weight_perturbation_tri = to_triangular(weight_perturbation)  # Martino\n",
    "        small_idx = np.argsort(copied_curvature_tri, axis=None)\n",
    "        weight_perturbation_tri[small_idx[:number_of_changed_values]] = 1\n",
    "        copied_curvature_tri[small_idx[:number_of_changed_values]] = 2000\n",
    "        weight_perturbation_v2 = from_triangular(IMAGE_SIZE**2, weight_perturbation_tri, 1)\n",
    "        copied_curvature_v2 = from_triangular(IMAGE_SIZE**2, copied_curvature_tri, 1000)\n",
    "            \n",
    "#         for i in range(number_of_changed_values):\n",
    "#             y_ind_current, x_ind_current = np.unravel_index(copied_curvature.argmin(), copied_curvature.shape)\n",
    "#             weight_perturbation[y_ind_current, x_ind_current] = 1\n",
    "#             weight_perturbation[x_ind_current, y_ind_current] = 1\n",
    "#             copied_curvature[y_ind_current, x_ind_current] = 2000\n",
    "#             copied_curvature[x_ind_current, y_ind_current] = 2000\n",
    "            \n",
    "#         print(np.sum(np.abs(weight_perturbation - weight_perturbation_v2)))\n",
    "        weight_perturbation = weight_perturbation_v2\n",
    "        copied_curvature = copied_curvature_v2\n",
    "\n",
    "\n",
    "        xyz = np.zeros(np.shape(w1))\n",
    "        xyz[weight_perturbation == 1] = w1[weight_perturbation == 1]\n",
    "        mean_w1_considered2 += np.mean(np.abs(xyz))\n",
    "        perturbation_vector = weight_perturbation * z\n",
    "        wF = wF + perturbation_vector\n",
    "        summed_perturbed2 += np.mean(np.abs(perturbation_vector))\n",
    "        netFisher.set_weights(wF)\n",
    "\n",
    "        overall_errorFI = np.zeros(NTRAIN)\n",
    "        for i in range(int(stored_patterns)):\n",
    "            netFisher.present_pattern(original_patterns[:,i])\n",
    "            netFisher.step(100)\n",
    "            output = netFisher.s\n",
    "            error[epoch] = np.sum((original_patterns[:,i]-output)**2)\n",
    "            error[epoch] = dice_coefficient(original_patterns[:,i], output)\n",
    "            overall_errorFI[epoch] += error[epoch]\n",
    "\n",
    "        netFisher.present_pattern(original_patterns[:,stored_patterns+1])\n",
    "        netFisher.step(100)\n",
    "        output = netFisher.s\n",
    "        error[epoch] = np.sum((original_patterns[:,stored_patterns+1]-output)**2)\n",
    "        error[epoch] = dice_coefficient(original_patterns[:,stored_patterns+1], output)\n",
    "\n",
    "        complete_error_meanFI[trial, epoch] = overall_errorFI[epoch]/stored_patterns\n",
    "        complete_error_new_patternFI[trial, epoch] = error[epoch]\n",
    "\n",
    "        #if epoch == (NTRAIN-1):\n",
    "        #    x = np.abs(netFisher.w).flatten()\n",
    "        #    netFisher.calculate_fisher_information(patterns[:,0:stored_patterns+1])\n",
    "        #    y = netFisher.curvature.flatten()\n",
    "        #    plt.figure(figsize=(5,5))\n",
    "        #    plt.plot(x,y, 'x')\n",
    "        #    plt.xlabel('abs(w)')\n",
    "        #    plt.ylabel('FisherInf')\n",
    "        #    plt.show()\n",
    "\n",
    "# ======== FIH ========= #    \n",
    "# Now disturbing the weights using hebbian way for fisher information\n",
    "    if RUN_FIH:\n",
    "        wF = w1\n",
    "        for epoch in range(NTRAIN):\n",
    "            z = ETA * (np.outer(patterns[:,stored_patterns+1], patterns[:,stored_patterns+1]) - wF)\n",
    "            #netFisher.calculate_fisher_information(patterns[:,0:stored_patterns+1])\n",
    "            netFisher.calculate_fisher_information_hebbian(patterns[:,0:stored_patterns])\n",
    "            weight_perturbation = less_changed_weight_value*np.ones(shape = np.shape(w1))\n",
    "            np.fill_diagonal(weight_perturbation, 1)\n",
    "            copied_curvature = copy.deepcopy(netFisher.curvature)\n",
    "            np.fill_diagonal(copied_curvature, 1000) #setting it to a very high value such that the diagonal is not touched\n",
    "            \n",
    "            copied_curvature_tri = to_triangular(copied_curvature) # Martino\n",
    "            weight_perturbation_tri = to_triangular(weight_perturbation)  # Martino\n",
    "            small_idx = np.argsort(copied_curvature_tri, axis=None)\n",
    "            weight_perturbation_tri[small_idx[:number_of_changed_values]] = 1\n",
    "            copied_curvature_tri[small_idx[:number_of_changed_values]] = 2000\n",
    "            weight_perturbation_v2 = from_triangular(IMAGE_SIZE**2, weight_perturbation_tri, 1)\n",
    "            copied_curvature_v2 = from_triangular(IMAGE_SIZE**2, copied_curvature_tri, 1000)\n",
    "            \n",
    "#             for i in range(number_of_changed_values):\n",
    "#                 y_ind_current, x_ind_current = np.unravel_index(copied_curvature.argmin(), copied_curvature.shape)\n",
    "#                 weight_perturbation[y_ind_current, x_ind_current] = 1\n",
    "#                 weight_perturbation[x_ind_current, y_ind_current] = 1\n",
    "#                 copied_curvature[y_ind_current, x_ind_current] = 2000\n",
    "#                 copied_curvature[x_ind_current, y_ind_current] = 2000\n",
    "#             print(np.sum(np.abs(weight_perturbation - weight_perturbation_v2)))\n",
    "            weight_perturbation = weight_perturbation_v2\n",
    "            copied_curvature = copied_curvature_v2\n",
    "\n",
    "\n",
    "            xyz = np.zeros(np.shape(w1))\n",
    "            xyz[weight_perturbation == 1] = w1[weight_perturbation == 1]\n",
    "            mean_w1_considered2 += np.mean(np.abs(xyz))\n",
    "            perturbation_vector = weight_perturbation * z\n",
    "            wF = wF + perturbation_vector\n",
    "            summed_perturbed2 += np.mean(np.abs(perturbation_vector))\n",
    "            netFisher.set_weights(wF)\n",
    "\n",
    "            overall_errorFIH = np.zeros(NTRAIN)\n",
    "            for i in range(int(stored_patterns)):\n",
    "                netFisher.present_pattern(original_patterns[:,i])\n",
    "                netFisher.step(100)\n",
    "                output = netFisher.s\n",
    "                error[epoch] = np.sum((original_patterns[:,i]-output)**2)\n",
    "                error[epoch] = dice_coefficient(original_patterns[:,i], output)\n",
    "                overall_errorFIH[epoch] += error[epoch]\n",
    "\n",
    "            netFisher.present_pattern(original_patterns[:,stored_patterns+1])\n",
    "            netFisher.step(100)\n",
    "            output = netFisher.s\n",
    "            error[epoch] = np.sum((original_patterns[:,stored_patterns+1]-output)**2)\n",
    "            error[epoch] = dice_coefficient(original_patterns[:,stored_patterns+1], output)\n",
    "\n",
    "            complete_error_meanFIH[trial, epoch] = overall_errorFIH[epoch]/stored_patterns\n",
    "            complete_error_new_patternFIH[trial, epoch] = error[epoch]\n",
    "\n",
    "            #if epoch == (NTRAIN-1):\n",
    "            #    x = np.abs(netFisher.w).flatten()\n",
    "            #    netFisher.calculate_fisher_information(patterns[:,0:stored_patterns+1])\n",
    "            #    y = netFisher.curvature.flatten()\n",
    "            #    plt.figure(figsize=(5,5))\n",
    "            #    plt.plot(x,y, 'x')\n",
    "            #    plt.xlabel('abs(w)')\n",
    "            #    plt.ylabel('FisherInf')\n",
    "            #    plt.show()\n",
    "\n",
    "        wFIH_final = netFisher.w\n",
    "print('**Finished**')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "savedict = {\n",
    "    'trad_80_O': complete_error_mean,\n",
    "    'trad_80_N': complete_error_new_pattern,\n",
    "    'FI_O': complete_error_meanFI,\n",
    "    'FI_N': complete_error_new_patternFI }\n",
    "\n",
    "keys = ['trad_80_O', 'trad_80_N'] \n",
    "\n",
    "if RUN_FL:\n",
    "    savedict['FL_O'] = complete_error_meanFL\n",
    "    savedict['FL_N'] = complete_error_new_patternFL\n",
    "    keys += ['FL_O', 'FL_N']\n",
    "if RUN_FLT:\n",
    "    savedict['FLT_O'] = complete_error_meanFLT\n",
    "    savedict['FLT_N'] = complete_error_new_patternFLT\n",
    "    keys += ['FLT_O', 'FLT_N']\n",
    "keys += ['FI_O', 'FI_N']\n",
    "if RUN_FIH:\n",
    "    savedict['FIH_O'] = complete_error_meanFIH\n",
    "    savedict['FIH_N'] = complete_error_new_patternFIH\n",
    "    keys += ['FIH_O', 'FIH_N']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = \"Complete_errors2.npz\"\n",
    "np.savez(filename, **savedict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display results\n",
    "### Which could be loaded independently of the code above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = \"../results/Complete_errors_stored5_size10_spars0.1_etasmall.npz\"\n",
    "savedict = np.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_stuff(what, label):\n",
    "    center = np.median(what, axis=0)\n",
    "    edges = np.percentile(what, [25, 75], axis=0)\n",
    "    plt.plot(center, label=label)\n",
    "    plt.fill_between(np.arange(len(center)), *edges, alpha=.3)\n",
    "    plt.ylabel('Pixel Error %')\n",
    "    plt.ylim([0, 1.1])\n",
    "    plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ndatasets = len(savedict.keys())\n",
    "for i, k in enumerate(savedict.keys()):\n",
    "    plt.subplot(ndatasets//2, 2, i+1)\n",
    "    plot_stuff(savedict[k], label=k)\n",
    "    \n",
    "plt.xlabel(\"n. Iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_stuff_together(what, label, axisn):\n",
    "    plt.subplot(1, 2, axisn)\n",
    "    center = np.median(what, axis=0)\n",
    "    edges = np.percentile(what, [25, 75], axis=0)\n",
    "    l = plt.plot(center, '.-', label=label, linewidth=2.5)\n",
    "    c = l[0].get_color()\n",
    "    plt.fill_between(np.arange(len(center)), *edges, alpha=.15, color=c)\n",
    "    plt.plot(edges.T, color=c, alpha=.2)\n",
    "    plt.ylabel('Pixel Error %')\n",
    "    plt.ylim([0, 1.1])\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.xlim([-1, 2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,8))\n",
    "\n",
    "for k in savedict.keys():\n",
    "    axisn = 2 if k[-1] == 'N' else 1\n",
    "    plot_stuff_together(savedict[k], label=k, axisn=axisn)\n",
    "\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title('Old pattern',fontweight='bold', fontsize=20)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Dice Coefficient')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('New pattern',fontweight='bold', fontsize=20)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Dice Coefficient')\n",
    "plt.xlim([100, 150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
